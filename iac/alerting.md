# Resource Alerting

## Where are we alerting?

- SNS, Slack

## Synthetics Alerting

- These are high level alerts to verify that the endpoint can take traffic.
- They are set up for the test and prod environments only since dev could be up and down quite a bit depending on how much development work is occurring.
- They are located in DataDog mainly because it's outside of AWS.
- There are 2 tests. One test is to verify we receive an http 200 on the /health endpoint. The other is make sure that the call takes less than 1000ms on the /health endpoint.

## What resources are being monitored and alerted upon?

- ALB
- ECS(Fargate)
- Kinesis Data Streams
- Kinesis Data Analytics
- Kinesis Firehose
- Timestream
- Lambda

## What metric are we alerting on by resource and why?

### Production(Dev and Test have different thresholds as it's traffic patterns are much smaller)

| Region    | Resource Namespace     | Description                                                                                | Metric Name                                    | # of Eval. Periods | Period      | Comparison Oper.              | Statistic | Threshold          | Action |
| --------- | ---------------------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------- | ------------------ | ----------- | ----------------------------- | --------- | ------------------ | ------ |
| us-east-1 | AWS/ApplicationELB     | Request count has crossed 10k per second                                                   | HTTPCode_Target_2XX_Count                      | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 660000 requests    | SNS    |
| us-east-1 | AWS/ApplicationELB     | Service is returning too many 5xx response                                                 | HTTPCode_Target_5XX_Count                      | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 500 requests       | SNS    |
| us-east-1 | AWS/ApplicationELB     | Service response time is high                                                              | TargetResponseTime                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 1 second           | SNS    |
| us-east-1 | ECS/ContainerInsights  | Number of running tasks is high                                                            | RunningTaskCount                               | 1                  | 128 seonds  | GreaterThanOrEqualToThreshold | Average   | 128 tasks          | SNS    |
| us-east-1 | AWS/ECS                | ECS CPU Utilization is high                                                                | CPUUtilization                                 | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 90 percent         | SNS    |
| us-east-1 | AWS/ECS                | ECS Memory Utilization is high                                                             | MemoryUtilization                              | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 90 percent         | SNS    |
| us-east-1 | AWS/Kinesis            | Raw KDS PutRecords FailedRecords is High                                                   | PutRecords.FailedRecords                       | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 500 failed records | SNS    |
| us-east-1 | AWS/Kinesis            | Agg KDS PutRecords FailedRecords is High                                                   | PutRecords.FailedRecords                       | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 100 failed records | SNS    |
| us-east-1 | KinesisProducerLibrary | High Number of Raw KPL Retries                                                             | RetriesPerRecord                               | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 500 retries        | SNS    |
| us-east-1 | KinesisProducerLibrary | High Number of Agg KPL Retries                                                             | RetriesPerRecord                               | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 100 retries        | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Downtime occurred in Aggregator Flink App                                                  | downtime                                       | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Downtime occurred in Buffer Flink App                                                      | downtime                                       | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Checkpoints Failed for Aggregator Flink App                                                | numberOfFailedCheckpoints                      | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Checkpoints Failed for Buffer Flink App                                                    | numberOfFailedCheckpoints                      | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Number of records written per second is below threshold for Aggregator Flink App           | numRecordsOutPerSecond                         | 1                  | 300 seconds | LessThanOrEqualToThreshold    | Average   | 2.50 records       | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Number of records written per second is below threshold for Buffer Flink App               | numRecordsOutPerSecond                         | 1                  | 300 seconds | LessThanOrEqualToThreshold    | Average   | .40 record         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Aggregator Flink App is behind the latest record                                           | millisBehindLatest                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 60000 milliseconds | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Buffer Flink App is behind the latest record                                               | millisBehindLatest                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 60000 milliseconds | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Checkpoint duration for Aggregator Flink App is taking longer than expected                | lastCheckpointDuration                         | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 2500 milliseconds  | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Checkpoint duration for Buffer Flink App is taking longer than expected                    | lastCheckpointDuration                         | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 3000 milliseconds  | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Checkpoint size for Aggregator Flink App has grown larger than expected                    | lastCheckpointSize                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 3 MB               | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Checkpoint size for Buffer Flink App has grown larger than expected                        | lastCheckpointSize                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 3 MB               | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Heap Memory Utilization for Aggregator Flink App is higher than expected                   | heapMemoryUtilization                          | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Heap Memory Utilization for Buffer Flink App is higher than expected                       | heapMemoryUtilization                          | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | CPU Utilization for Aggregator Flink App is higher than expected                           | cpuUtilization                                 | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | CPU Utilization for Buffer Flink App is higher than expected                               | cpuUtilization                                 | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Thread Count for Aggregator Flink App is higher than expected                              | threadsCount                                   | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | Thread Count for Buffer Flink App is higher than expected                                  | threadsCount                                   | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | The GarbageCollectionTime exceeded the maximum expected for the Aggregator Flink App       | (oldGenerationGCTime \* 100) / 60              | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | The GarbageCollectionTime exceeded the maximum expected for the Buffer Flink App           | (oldGenerationGCTime \* 100) / 60              | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | The GarbageCollectionCount Rate exceeded the maximum expected for the Aggregator Flink App | RATE(oldGenerationGCCount)                     | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | The GarbageCollectionCount Rate exceeded the maximum expected for the Buffer Flink App     | RATE(oldGenerationGCCount)                     | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | The Aggregator Flink App is processing increasingly older events                           | currentOutputWatermark - currentInputWatermark | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90                 | SNS    |
| us-east-1 | AWS/KinesisAnalytics   | The Buffer Flink App is processing increasingly older events                               | currentOutputWatermark - currentInputWatermark | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90                 | SNS    |
| us-east-1 | AWS/Timestream         | System Errors exceeded threshold                                                           | SystemErrors                                   | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 50 errors          | SNS    |
| us-east-1 | AWS/Timestream         | User Errors exceeded threshold                                                             | UserErrors                                     | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 50 errors          | SNS    |
| us-east-1 | AWS/Timestream         | Successful Request Latency for WriteRecords is taking longer than expected                 | SuccessfulRequestLatency                       | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 100 milliseconds   | SNS    |
| us-west-2 | AWS/ApplicationELB     | Request count has crossed 10k per second                                                   | HTTPCode_Target_2XX_Count                      | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 660000 requests    | SNS    |
| us-west-2 | AWS/ApplicationELB     | Service is returning too many 5xx response                                                 | HTTPCode_Target_5XX_Count                      | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 500 requests       | SNS    |
| us-west-2 | AWS/ApplicationELB     | Service response time is high                                                              | TargetResponseTime                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 1 second           | SNS    |
| us-west-2 | ECS/ContainerInsights  | Number of running tasks is high                                                            | RunningTaskCount                               | 1                  | 128 seonds  | GreaterThanOrEqualToThreshold | Average   | 128 tasks          | SNS    |
| us-west-2 | AWS/ECS                | ECS CPU Utilization is high                                                                | CPUUtilization                                 | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 90 percent         | SNS    |
| us-west-2 | AWS/ECS                | ECS Memory Utilization is high                                                             | MemoryUtilization                              | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 90 percent         | SNS    |
| us-west-2 | AWS/Kinesis            | Raw KDS PutRecords FailedRecords is High                                                   | PutRecords.FailedRecords                       | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 500 failed records | SNS    |
| us-west-2 | AWS/Kinesis            | Agg KDS PutRecords FailedRecords is High                                                   | PutRecords.FailedRecords                       | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 100 failed records | SNS    |
| us-west-2 | KinesisProducerLibrary | High Number of Raw KPL Retries                                                             | RetriesPerRecord                               | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 500 retries        | SNS    |
| us-west-2 | KinesisProducerLibrary | High Number of Agg KPL Retries                                                             | RetriesPerRecord                               | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Sum       | 100 retries        | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Downtime occurred in Aggregator Flink App                                                  | downtime                                       | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Downtime occurred in Buffer Flink App                                                      | downtime                                       | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Checkpoints Failed for Aggregator Flink App                                                | numberOfFailedCheckpoints                      | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Checkpoints Failed for Buffer Flink App                                                    | numberOfFailedCheckpoints                      | 1                  | 300 seconds | GreaterThanThreshold          | Average   | 0                  | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Number of records written per second is below threshold for Aggregator Flink App           | numRecordsOutPerSecond                         | 1                  | 300 seconds | LessThanOrEqualToThreshold    | Average   | .60 records        | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Number of records written per second is below threshold for Buffer Flink App               | numRecordsOutPerSecond                         | 1                  | 300 seconds | LessThanOrEqualToThreshold    | Average   | .20 record         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Aggregator Flink App is behind the latest record                                           | millisBehindLatest                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 60000 milliseconds | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Buffer Flink App is behind the latest record                                               | millisBehindLatest                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Average   | 60000 milliseconds | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Checkpoint duration for Aggregator Flink App is taking longer than expected                | lastCheckpointDuration                         | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 2000 milliseconds  | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Checkpoint duration for Buffer Flink App is taking longer than expected                    | lastCheckpointDuration                         | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 2000 milliseconds  | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Checkpoint size for Aggregator Flink App has grown larger than expected                    | lastCheckpointSize                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 3 MB               | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Checkpoint size for Buffer Flink App has grown larger than expected                        | lastCheckpointSize                             | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 3 MB               | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Heap Memory Utilization for Aggregator Flink App is higher than expected                   | heapMemoryUtilization                          | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Heap Memory Utilization for Buffer Flink App is higher than expected                       | heapMemoryUtilization                          | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | CPU Utilization for Aggregator Flink App is higher than expected                           | cpuUtilization                                 | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | CPU Utilization for Buffer Flink App is higher than expected                               | cpuUtilization                                 | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Thread Count for Aggregator Flink App is higher than expected                              | threadsCount                                   | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | Thread Count for Buffer Flink App is higher than expected                                  | threadsCount                                   | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | The GarbageCollectionTime exceeded the maximum expected for the Aggregator Flink App       | (oldGenerationGCTime \* 100) / 60              | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | The GarbageCollectionTime exceeded the maximum expected for the Buffer Flink App           | (oldGenerationGCTime \* 100) / 60              | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | The GarbageCollectionCount Rate exceeded the maximum expected for the Aggregator Flink App | RATE(oldGenerationGCCount)                     | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | The GarbageCollectionCount Rate exceeded the maximum expected for the Buffer Flink App     | RATE(oldGenerationGCCount)                     | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90 percent         | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | The Aggregator Flink App is processing increasingly older events                           | currentOutputWatermark - currentInputWatermark | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90                 | SNS    |
| us-west-2 | AWS/KinesisAnalytics   | The Buffer Flink App is processing increasingly older events                               | currentOutputWatermark - currentInputWatermark | 1                  | 60 seconds  | GreaterThanOrEqualToThreshold | Maximum   | 90                 | SNS    |
| us-west-2 | AWS/Timestream         | System Errors exceeded threshold                                                           | SystemErrors                                   | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 50 errors          | SNS    |
| us-west-2 | AWS/Timestream         | User Errors exceeded threshold                                                             | UserErrors                                     | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 50 errors          | SNS    |
| us-west-2 | AWS/Timestream         | Successful Request Latency for WriteRecords is taking longer than expected                 | SuccessfulRequestLatency                       | 1                  | 300 seconds | GreaterThanOrEqualToThreshold | Maximum   | 100 milliseconds   | SNS    |
